<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>quaterion.train.cache_mixin API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>quaterion.train.cache_mixin</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import os
import multiprocessing as mp

from typing import (
    Union,
    Dict,
    Optional,
    Set,
    Any,
    Callable,
    Hashable,
    Iterable,
    Tuple,
)

import torch.cuda
import pytorch_lightning as pl

from loguru import logger
from torch.utils.data import DataLoader
from pytorch_lightning.loops import (
    FitLoop,
    TrainingEpochLoop,
    TrainingBatchLoop,
    EvaluationLoop,
)
from pytorch_lightning.utilities.types import (
    TRAIN_DATALOADERS,
    EVAL_DATALOADERS,
)
from quaterion_models.encoders import Encoder
from quaterion_models.model import DEFAULT_ENCODER_KEY
from quaterion_models.types import TensorInterchange

from quaterion.dataset.cache_data_loader import CacheDataLoader
from quaterion.train.encoders.cache_encoder import KeyExtractorType
from quaterion.dataset.similarity_data_loader import SimilarityDataLoader
from quaterion.train.encoders import (
    CacheConfig,
    CacheEncoder,
    CacheType,
    InMemoryCacheEncoder,
)


class CacheMixin:
    CACHE_MULTIPROCESSING_CONTEXT = &#34;fork&#34;

    @classmethod
    def _apply_cache_config(
        cls,
        encoders: Union[Encoder, Dict[str, Encoder]],
        cache_config: Optional[CacheConfig],
    ) -&gt; Union[Encoder, Dict[str, Encoder]]:
        &#34;&#34;&#34;Applies received cache configuration for cached encoders, remain
        non-cached encoders as is

        Args:
            encoders: all model&#39;s encoders
            cache_config: CacheConfig instance defined in `configure_cache`
                method of the model

        Returns:
            Union[Encoder, Dict[str, encoder]]: encoder or dict of encoders
                which were wrapped into CacheEncoder instances according to
                received cache config.
                Result type depends on the way encoder was defined in the
                model: with or without explicit mapping

        Raises:
            KeyError: encoder&#39;s name in cache config is not in model&#39;s
                encoders
            ValueError: if CacheConfig instance does not have some of required
                options set. E.g. not `mapping` nor `cache_type` being set
        &#34;&#34;&#34;
        if not cache_config:
            return encoders

        if cache_config.mapping:
            if cache_config.cache_type:
                logger.warning(
                    &#34;CacheConfig.cache_type has no effect when mapping is set&#34;
                )

            possible_cache_encoders: Set[str] = {
                encoder_name
                for encoder_name in encoders
                if not encoders[encoder_name].trainable()
            }

            for encoder_name, cache_type in cache_config.mapping.items():
                encoder: Optional[Encoder] = encoders.get(encoder_name)
                if not encoder:
                    raise KeyError(
                        f&#34;Can&#39;t configure cache for encoder {encoder_name}. &#34;
                        &#34;Encoder not found&#34;
                    )
                cls._check_cuda(cache_type, encoder_name)
                key_extractor: Optional[
                    Callable[[Any], Hashable]
                ] = cache_config.key_extractors.get(encoder_name)

                encoders[encoder_name]: CacheEncoder = cls._wrap_encoder(
                    encoder,
                    cache_type,
                    key_extractor,
                    encoder_name,
                )

                possible_cache_encoders.remove(encoder_name)

            not_cached_encoders = &#34;, &#34;.join(possible_cache_encoders)
            if not_cached_encoders:
                logger.info(
                    f&#34;{not_cached_encoders} haven&#39;t been cached, &#34;
                    &#34;but could be as non-trainable encoders&#34;
                )

        elif cache_config.cache_type:
            encoder_name = DEFAULT_ENCODER_KEY

            cls._check_cuda(cache_config.cache_type, encoder_name)
            key_extractor = cache_config.key_extractors.get(encoder_name)
            encoders = cls._wrap_encoder(
                encoders,
                cache_config.cache_type,
                key_extractor,
                encoder_name,
            )
        else:
            raise ValueError(
                &#34;If cache is configured, cache_type or mapping have to be set&#34;
            )

        return encoders

    @staticmethod
    def _check_cuda(cache_type: CacheType, encoder_name: str) -&gt; None:
        &#34;&#34;&#34;Check cuda availability if GPU was chosen as cached tensors storage.

        If encoder is supposed to use GPU as tensor&#39;s storage, then cuda
        has to be available, otherwise raises ValueError

        Args:
            cache_type: cache type for encoder
            encoder_name: name of cache encoder

        Raises:
            ValueError: If `CacheType.GPU` was passed and cuda is not available
        &#34;&#34;&#34;
        if cache_type == CacheType.GPU and not torch.cuda.is_available():
            raise ValueError(
                f&#34;`CacheType.GPU` has been chosen for `{encoder_name}` &#34;
                &#34;encoder, but cuda is not available&#34;
            )

    @staticmethod
    def _wrap_encoder(
        encoder: Encoder,
        cache_type: CacheType,
        key_extractor: Optional[KeyExtractorType],
        encoder_name: str = &#34;&#34;,
    ) -&gt; CacheEncoder:
        &#34;&#34;&#34;Wrap encoder into CacheEncoder instance.

        Args:
            encoder: raw model&#39;s encoder
            cache_type: cache type of tensor storage
            key_extractor: function to obtain hashable values for complex
                object which can&#39;t be hashed with standard means.
            encoder_name: name of encoder to be wrapped

        Returns:
            CacheEncoder: wrapped encoder

        Raises:
            ValueError: if encoder layers are not frozen. Cache can be applied
                only to fully frozen encoders&#39; outputs.
        &#34;&#34;&#34;
        if encoder.trainable():
            raise ValueError(
                f&#34;Can&#39;t configure cache for encoder {encoder_name}. &#34;
                &#34;Encoder must be frozen to cache it&#34;
            )

        encoder = InMemoryCacheEncoder(encoder, key_extractor, cache_type)

        return encoder

    @classmethod
    def cache(
        cls,
        trainer: pl.Trainer,
        encoders: Dict[str, Encoder],
        train_dataloader: SimilarityDataLoader,
        val_dataloader: Optional[SimilarityDataLoader],
        cache_config: CacheConfig,
    ) -&gt; None:
        &#34;&#34;&#34;Filling cache for model&#39;s cache encoders.

        Args:
            trainer: performs one training epoch to cache encoders outputs.
                Preserve all encapsulated pytorch-lightning logic such as
                device managing etc.
            encoders: mapping of model&#39;s encoders and their names
            train_dataloader: model&#39;s train dataloader
            val_dataloader: model&#39;s val dataloader
            cache_config: cache config instance to configure cache batch size
                and num of workers to use for caching

        &#34;&#34;&#34;
        cache_encoders = {
            name: encoder
            for name, encoder in encoders.items()
            if isinstance(encoder, CacheEncoder)
        }

        if not cache_encoders:
            return

        cls._compatibility_check(train_dataloader)
        train_dataloader = cls._wrap_cache_dataloader(
            train_dataloader, cache_config, cache_encoders
        )

        if val_dataloader is not None:
            val_dataloader = cls._wrap_cache_dataloader(
                val_dataloader, cache_config, cache_encoders
            )

        cls._fill_cache(trainer, cache_encoders, train_dataloader, val_dataloader)

        # Once cache is filled, collate functions return only keys for cache
        for encoder in cache_encoders.values():
            encoder.cache_filled = True
        logger.info(&#34;Caching has been successfully finished&#34;)

    @classmethod
    def _fill_cache(
        cls,
        trainer: pl.Trainer,
        cache_encoders: Dict[str, CacheEncoder],
        train_dataloader: SimilarityDataLoader,
        val_dataloader: SimilarityDataLoader,
    ) -&gt; None:
        &#34;&#34;&#34;Fills cache and restores trainer state for further training process.

        Args:
            trainer: performs one training and validation epoch
            cache_encoders: mapping of encoders to cache input
            train_dataloader: model&#39;s train dataloader
            val_dataloader: model&#39;s val dataloader

        &#34;&#34;&#34;
        # store configured fit and validate loops to restore them for training
        # process after cache
        _fit_loop = trainer.fit_loop
        _val_loop = trainer.validate_loop

        # Mimic fit loop configuration from trainer
        fit_loop = FitLoop(
            min_epochs=1,
            max_epochs=1,
        )
        training_epoch_loop = TrainingEpochLoop()
        training_batch_loop = TrainingBatchLoop()
        training_validation_loop = EvaluationLoop()
        training_epoch_loop.connect(
            batch_loop=training_batch_loop, val_loop=training_validation_loop
        )
        fit_loop.connect(epoch_loop=training_epoch_loop)
        trainer.fit_loop = fit_loop

        # The actual caching
        trainer.predict(
            CacheModel(
                cache_encoders,
            ),
            [train_dataloader, val_dataloader],
        )
        trainer.fit_loop = _fit_loop

    @classmethod
    def _compatibility_check(cls, dataloader: DataLoader) -&gt; None:
        &#34;&#34;&#34;Checks whether dataloader type is cacheable or not.

        To be used in caching, dataloader has to have an implementation of
        `cache_collate_fn`. Currently, only `SimilarityDataLoader` has it.

        Args:
            dataloader: DataLoader

        Raises:
            TypeError: if dataloader is not instance of `SimilarityDataLoader`
        &#34;&#34;&#34;
        if not isinstance(dataloader, SimilarityDataLoader):
            raise TypeError(&#34;DataLoader must be SimilarityDataLoader &#34;)

    @classmethod
    def _wrap_cache_dataloader(
        cls,
        dataloader: SimilarityDataLoader,
        cache_config: CacheConfig,
        cache_encoders: Dict[str, CacheEncoder],
    ) -&gt; CacheDataLoader:
        &#34;&#34;&#34;Wrap dataloader for caching.

        Child processes need to derive randomized `PYTHONHASHSEED` value from
        parent process to obtain the same hash values. It is only done with
        `fork` start method.
        `fork` start method is presented on Unix systems, and it is the default
        for them, except macOS. Therefore, we set `fork` method explicitly.

        If dataloader is not supposed to use child process, nothing being done.
        If multiprocessing_context was set by user, it is being untouched and
        can lead to errors.
        If `PYTHONHASHSEED` is set explicitly then multiprocessing_context
        won&#39;t be switched.
        Cache can be used on Windows only in case when `PYTHONHASHSEED` is set
        explicitly.

        Args:
            dataloader: dataloader to be wrapped
            cache_config: cache config to retrieve num of workers and batch
                size
            cache_encoders: encoders to set key extractors and collate_fns

        Returns:
            CacheDataLoader: wrapped dataloader
        &#34;&#34;&#34;
        cls._switch_multiprocessing_context(dataloader)
        num_workers = (
            cache_config.num_workers
            if cache_config.num_workers is not None
            else dataloader.num_workers
        )

        if num_workers == 0:
            mp_ctx = None
        elif dataloader.multiprocessing_context:  # already switched or
            # set by user
            mp_ctx = dataloader.multiprocessing_context
        elif &#34;PYTHONHASHSEED&#34; in os.environ:  # source dataloader has no
            # mp context set, use default on current OS
            mp_ctx = mp.get_start_method()
        else:
            mp_ctx = cls.CACHE_MULTIPROCESSING_CONTEXT
            cls._check_mp_context(mp_ctx)

        # We need to reduce random sampling and repeated calculations to
        # make cache as fast as possible. Thus, we recreate dataloader
        # and set batch size explicitly.
        cache_dl = CacheDataLoader(
            key_extractors=dict(
                (encoder_name, cache_encoder.key_extractor)
                for encoder_name, cache_encoder in cache_encoders.items()
            ),
            cached_encoders_collate_fns=dict(
                (encoder_name, cache_encoder.get_collate_fn())
                for encoder_name, cache_encoder in cache_encoders.items()
            ),
            unique_objects_extractor=dataloader.fetch_unique_objects,
            dataset=dataloader.dataset,
            batch_size=cache_config.batch_size,
            num_workers=num_workers,
            pin_memory=dataloader.pin_memory,
            timeout=dataloader.timeout,
            worker_init_fn=dataloader.worker_init_fn,
            prefetch_factor=dataloader.prefetch_factor,
            multiprocessing_context=mp_ctx,
        )
        return cache_dl

    @classmethod
    def _switch_multiprocessing_context(cls, dataloader: SimilarityDataLoader) -&gt; None:
        &#34;&#34;&#34;Switch dataloader multiprocessing context.

        Do nothing if dataloader is not supposed to use child processes or
        `PYTHONHASHSEED` has been set explicitly by user.

        Args:
            dataloader: dataloader to check and switch multiprocessing context

        &#34;&#34;&#34;
        if &#34;PYTHONHASHSEED&#34; in os.environ:
            return

        if dataloader.num_workers == 0:
            return

        mp_context: Optional[
            Union[str, mp.context.BaseContext]
        ] = dataloader.multiprocessing_context
        cls._check_mp_context(mp_context)

        dataloader.multiprocessing_context = cls.CACHE_MULTIPROCESSING_CONTEXT

    @classmethod
    def _check_mp_context(
        cls, mp_context: Optional[Union[str, mp.context.BaseContext]]
    ) -&gt; None:
        &#34;&#34;&#34;Check if multiprocessing context is compatible with cache.

        Emits warning if current multiprocessing context start method does not
        coincide with one required by cache.

        Args:
            mp_context: some dataloader&#39;s multiprocessing context

        Raises:
            OSError: Raise OSError if OS does not support process start method
            required by cache. Currently, this start method is `fork` which is
            not supported by Windows.
        &#34;&#34;&#34;
        if not mp_context:
            return

        if not isinstance(mp_context, str):
            mp_context = mp_context.get_start_method()

        if mp_context != cls.CACHE_MULTIPROCESSING_CONTEXT:
            logger.warning(
                &#34;Default start method on your OS is not &#34;
                f&#34;{cls.CACHE_MULTIPROCESSING_CONTEXT}. &#34;
                &#34;Trying to switch it. However &#34;
                f&#34;{cls.CACHE_MULTIPROCESSING_CONTEXT} may be unsafe or &#34;
                &#34;unsupported. The most safe option is to launch your process &#34;
                &#34;with fixed `PYTHONHASHSEED` env and remain `spawn` as start &#34;
                &#34;method.\n&#34;
                &#34;Possible launch is `PYTHONHASHSEED=0 python3 run.py&#34;
            )

        if cls.CACHE_MULTIPROCESSING_CONTEXT not in mp.get_all_start_methods():
            raise OSError(
                f&#34;Cache can&#39;t be used without setting `PYTHONHASHSEED`. &#34;
                f&#34;{cls.CACHE_MULTIPROCESSING_CONTEXT} multiprocessing context &#34;
                &#34;is not available on current OS&#34;
            )


class CacheModel(pl.LightningModule):
    &#34;&#34;&#34;Mock model for convenient caching.

    This class is required to make caching process similar to the training of
    the genuine model and inherit and use the same trainer instance. It allows
    avoiding of messing with device managing stuff and more.

    Args:
        encoders: dict of cache encoders names and corresponding instances to cache
    &#34;&#34;&#34;

    def __init__(
        self,
        encoders: Dict[str, CacheEncoder],
    ):

        super().__init__()
        self.encoders = encoders
        for key, encoder in self.encoders.items():
            self.add_module(key, encoder)

    def predict_step(
        self,
        batch: Dict[str, Tuple[Iterable[Hashable], TensorInterchange]],
        batch_idx: int,
        dataloader_idx: Optional[int] = None,
    ):
        &#34;&#34;&#34;Caches batch of input.

        Args:
            batch: batch of collated data. Contains mapping, where key is
                encoder&#39;s name, value is tuple of key used in cache and
                according items, which are ready to be processed by specific
                encoder.
            batch_idx: Index of current batch
            dataloader_idx: Index of the current dataloader
        Returns:
            torch.Tensor: loss mock
        &#34;&#34;&#34;
        for encoder_name, encoder in self.encoders.items():
            encoder_samples = batch.get(encoder_name)
            if not encoder_samples:
                continue
            encoder.fill_cache(encoder_samples)

        return torch.Tensor([1])

    # region anchors
    def train_dataloader(self) -&gt; TRAIN_DATALOADERS:
        pass

    def test_dataloader(self) -&gt; EVAL_DATALOADERS:
        pass

    def val_dataloader(self) -&gt; EVAL_DATALOADERS:
        pass

    def predict_dataloader(self) -&gt; EVAL_DATALOADERS:
        pass

    # endregion</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="quaterion.train.cache_mixin.CacheMixin"><code class="flex name class">
<span>class <span class="ident">CacheMixin</span></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CacheMixin:
    CACHE_MULTIPROCESSING_CONTEXT = &#34;fork&#34;

    @classmethod
    def _apply_cache_config(
        cls,
        encoders: Union[Encoder, Dict[str, Encoder]],
        cache_config: Optional[CacheConfig],
    ) -&gt; Union[Encoder, Dict[str, Encoder]]:
        &#34;&#34;&#34;Applies received cache configuration for cached encoders, remain
        non-cached encoders as is

        Args:
            encoders: all model&#39;s encoders
            cache_config: CacheConfig instance defined in `configure_cache`
                method of the model

        Returns:
            Union[Encoder, Dict[str, encoder]]: encoder or dict of encoders
                which were wrapped into CacheEncoder instances according to
                received cache config.
                Result type depends on the way encoder was defined in the
                model: with or without explicit mapping

        Raises:
            KeyError: encoder&#39;s name in cache config is not in model&#39;s
                encoders
            ValueError: if CacheConfig instance does not have some of required
                options set. E.g. not `mapping` nor `cache_type` being set
        &#34;&#34;&#34;
        if not cache_config:
            return encoders

        if cache_config.mapping:
            if cache_config.cache_type:
                logger.warning(
                    &#34;CacheConfig.cache_type has no effect when mapping is set&#34;
                )

            possible_cache_encoders: Set[str] = {
                encoder_name
                for encoder_name in encoders
                if not encoders[encoder_name].trainable()
            }

            for encoder_name, cache_type in cache_config.mapping.items():
                encoder: Optional[Encoder] = encoders.get(encoder_name)
                if not encoder:
                    raise KeyError(
                        f&#34;Can&#39;t configure cache for encoder {encoder_name}. &#34;
                        &#34;Encoder not found&#34;
                    )
                cls._check_cuda(cache_type, encoder_name)
                key_extractor: Optional[
                    Callable[[Any], Hashable]
                ] = cache_config.key_extractors.get(encoder_name)

                encoders[encoder_name]: CacheEncoder = cls._wrap_encoder(
                    encoder,
                    cache_type,
                    key_extractor,
                    encoder_name,
                )

                possible_cache_encoders.remove(encoder_name)

            not_cached_encoders = &#34;, &#34;.join(possible_cache_encoders)
            if not_cached_encoders:
                logger.info(
                    f&#34;{not_cached_encoders} haven&#39;t been cached, &#34;
                    &#34;but could be as non-trainable encoders&#34;
                )

        elif cache_config.cache_type:
            encoder_name = DEFAULT_ENCODER_KEY

            cls._check_cuda(cache_config.cache_type, encoder_name)
            key_extractor = cache_config.key_extractors.get(encoder_name)
            encoders = cls._wrap_encoder(
                encoders,
                cache_config.cache_type,
                key_extractor,
                encoder_name,
            )
        else:
            raise ValueError(
                &#34;If cache is configured, cache_type or mapping have to be set&#34;
            )

        return encoders

    @staticmethod
    def _check_cuda(cache_type: CacheType, encoder_name: str) -&gt; None:
        &#34;&#34;&#34;Check cuda availability if GPU was chosen as cached tensors storage.

        If encoder is supposed to use GPU as tensor&#39;s storage, then cuda
        has to be available, otherwise raises ValueError

        Args:
            cache_type: cache type for encoder
            encoder_name: name of cache encoder

        Raises:
            ValueError: If `CacheType.GPU` was passed and cuda is not available
        &#34;&#34;&#34;
        if cache_type == CacheType.GPU and not torch.cuda.is_available():
            raise ValueError(
                f&#34;`CacheType.GPU` has been chosen for `{encoder_name}` &#34;
                &#34;encoder, but cuda is not available&#34;
            )

    @staticmethod
    def _wrap_encoder(
        encoder: Encoder,
        cache_type: CacheType,
        key_extractor: Optional[KeyExtractorType],
        encoder_name: str = &#34;&#34;,
    ) -&gt; CacheEncoder:
        &#34;&#34;&#34;Wrap encoder into CacheEncoder instance.

        Args:
            encoder: raw model&#39;s encoder
            cache_type: cache type of tensor storage
            key_extractor: function to obtain hashable values for complex
                object which can&#39;t be hashed with standard means.
            encoder_name: name of encoder to be wrapped

        Returns:
            CacheEncoder: wrapped encoder

        Raises:
            ValueError: if encoder layers are not frozen. Cache can be applied
                only to fully frozen encoders&#39; outputs.
        &#34;&#34;&#34;
        if encoder.trainable():
            raise ValueError(
                f&#34;Can&#39;t configure cache for encoder {encoder_name}. &#34;
                &#34;Encoder must be frozen to cache it&#34;
            )

        encoder = InMemoryCacheEncoder(encoder, key_extractor, cache_type)

        return encoder

    @classmethod
    def cache(
        cls,
        trainer: pl.Trainer,
        encoders: Dict[str, Encoder],
        train_dataloader: SimilarityDataLoader,
        val_dataloader: Optional[SimilarityDataLoader],
        cache_config: CacheConfig,
    ) -&gt; None:
        &#34;&#34;&#34;Filling cache for model&#39;s cache encoders.

        Args:
            trainer: performs one training epoch to cache encoders outputs.
                Preserve all encapsulated pytorch-lightning logic such as
                device managing etc.
            encoders: mapping of model&#39;s encoders and their names
            train_dataloader: model&#39;s train dataloader
            val_dataloader: model&#39;s val dataloader
            cache_config: cache config instance to configure cache batch size
                and num of workers to use for caching

        &#34;&#34;&#34;
        cache_encoders = {
            name: encoder
            for name, encoder in encoders.items()
            if isinstance(encoder, CacheEncoder)
        }

        if not cache_encoders:
            return

        cls._compatibility_check(train_dataloader)
        train_dataloader = cls._wrap_cache_dataloader(
            train_dataloader, cache_config, cache_encoders
        )

        if val_dataloader is not None:
            val_dataloader = cls._wrap_cache_dataloader(
                val_dataloader, cache_config, cache_encoders
            )

        cls._fill_cache(trainer, cache_encoders, train_dataloader, val_dataloader)

        # Once cache is filled, collate functions return only keys for cache
        for encoder in cache_encoders.values():
            encoder.cache_filled = True
        logger.info(&#34;Caching has been successfully finished&#34;)

    @classmethod
    def _fill_cache(
        cls,
        trainer: pl.Trainer,
        cache_encoders: Dict[str, CacheEncoder],
        train_dataloader: SimilarityDataLoader,
        val_dataloader: SimilarityDataLoader,
    ) -&gt; None:
        &#34;&#34;&#34;Fills cache and restores trainer state for further training process.

        Args:
            trainer: performs one training and validation epoch
            cache_encoders: mapping of encoders to cache input
            train_dataloader: model&#39;s train dataloader
            val_dataloader: model&#39;s val dataloader

        &#34;&#34;&#34;
        # store configured fit and validate loops to restore them for training
        # process after cache
        _fit_loop = trainer.fit_loop
        _val_loop = trainer.validate_loop

        # Mimic fit loop configuration from trainer
        fit_loop = FitLoop(
            min_epochs=1,
            max_epochs=1,
        )
        training_epoch_loop = TrainingEpochLoop()
        training_batch_loop = TrainingBatchLoop()
        training_validation_loop = EvaluationLoop()
        training_epoch_loop.connect(
            batch_loop=training_batch_loop, val_loop=training_validation_loop
        )
        fit_loop.connect(epoch_loop=training_epoch_loop)
        trainer.fit_loop = fit_loop

        # The actual caching
        trainer.predict(
            CacheModel(
                cache_encoders,
            ),
            [train_dataloader, val_dataloader],
        )
        trainer.fit_loop = _fit_loop

    @classmethod
    def _compatibility_check(cls, dataloader: DataLoader) -&gt; None:
        &#34;&#34;&#34;Checks whether dataloader type is cacheable or not.

        To be used in caching, dataloader has to have an implementation of
        `cache_collate_fn`. Currently, only `SimilarityDataLoader` has it.

        Args:
            dataloader: DataLoader

        Raises:
            TypeError: if dataloader is not instance of `SimilarityDataLoader`
        &#34;&#34;&#34;
        if not isinstance(dataloader, SimilarityDataLoader):
            raise TypeError(&#34;DataLoader must be SimilarityDataLoader &#34;)

    @classmethod
    def _wrap_cache_dataloader(
        cls,
        dataloader: SimilarityDataLoader,
        cache_config: CacheConfig,
        cache_encoders: Dict[str, CacheEncoder],
    ) -&gt; CacheDataLoader:
        &#34;&#34;&#34;Wrap dataloader for caching.

        Child processes need to derive randomized `PYTHONHASHSEED` value from
        parent process to obtain the same hash values. It is only done with
        `fork` start method.
        `fork` start method is presented on Unix systems, and it is the default
        for them, except macOS. Therefore, we set `fork` method explicitly.

        If dataloader is not supposed to use child process, nothing being done.
        If multiprocessing_context was set by user, it is being untouched and
        can lead to errors.
        If `PYTHONHASHSEED` is set explicitly then multiprocessing_context
        won&#39;t be switched.
        Cache can be used on Windows only in case when `PYTHONHASHSEED` is set
        explicitly.

        Args:
            dataloader: dataloader to be wrapped
            cache_config: cache config to retrieve num of workers and batch
                size
            cache_encoders: encoders to set key extractors and collate_fns

        Returns:
            CacheDataLoader: wrapped dataloader
        &#34;&#34;&#34;
        cls._switch_multiprocessing_context(dataloader)
        num_workers = (
            cache_config.num_workers
            if cache_config.num_workers is not None
            else dataloader.num_workers
        )

        if num_workers == 0:
            mp_ctx = None
        elif dataloader.multiprocessing_context:  # already switched or
            # set by user
            mp_ctx = dataloader.multiprocessing_context
        elif &#34;PYTHONHASHSEED&#34; in os.environ:  # source dataloader has no
            # mp context set, use default on current OS
            mp_ctx = mp.get_start_method()
        else:
            mp_ctx = cls.CACHE_MULTIPROCESSING_CONTEXT
            cls._check_mp_context(mp_ctx)

        # We need to reduce random sampling and repeated calculations to
        # make cache as fast as possible. Thus, we recreate dataloader
        # and set batch size explicitly.
        cache_dl = CacheDataLoader(
            key_extractors=dict(
                (encoder_name, cache_encoder.key_extractor)
                for encoder_name, cache_encoder in cache_encoders.items()
            ),
            cached_encoders_collate_fns=dict(
                (encoder_name, cache_encoder.get_collate_fn())
                for encoder_name, cache_encoder in cache_encoders.items()
            ),
            unique_objects_extractor=dataloader.fetch_unique_objects,
            dataset=dataloader.dataset,
            batch_size=cache_config.batch_size,
            num_workers=num_workers,
            pin_memory=dataloader.pin_memory,
            timeout=dataloader.timeout,
            worker_init_fn=dataloader.worker_init_fn,
            prefetch_factor=dataloader.prefetch_factor,
            multiprocessing_context=mp_ctx,
        )
        return cache_dl

    @classmethod
    def _switch_multiprocessing_context(cls, dataloader: SimilarityDataLoader) -&gt; None:
        &#34;&#34;&#34;Switch dataloader multiprocessing context.

        Do nothing if dataloader is not supposed to use child processes or
        `PYTHONHASHSEED` has been set explicitly by user.

        Args:
            dataloader: dataloader to check and switch multiprocessing context

        &#34;&#34;&#34;
        if &#34;PYTHONHASHSEED&#34; in os.environ:
            return

        if dataloader.num_workers == 0:
            return

        mp_context: Optional[
            Union[str, mp.context.BaseContext]
        ] = dataloader.multiprocessing_context
        cls._check_mp_context(mp_context)

        dataloader.multiprocessing_context = cls.CACHE_MULTIPROCESSING_CONTEXT

    @classmethod
    def _check_mp_context(
        cls, mp_context: Optional[Union[str, mp.context.BaseContext]]
    ) -&gt; None:
        &#34;&#34;&#34;Check if multiprocessing context is compatible with cache.

        Emits warning if current multiprocessing context start method does not
        coincide with one required by cache.

        Args:
            mp_context: some dataloader&#39;s multiprocessing context

        Raises:
            OSError: Raise OSError if OS does not support process start method
            required by cache. Currently, this start method is `fork` which is
            not supported by Windows.
        &#34;&#34;&#34;
        if not mp_context:
            return

        if not isinstance(mp_context, str):
            mp_context = mp_context.get_start_method()

        if mp_context != cls.CACHE_MULTIPROCESSING_CONTEXT:
            logger.warning(
                &#34;Default start method on your OS is not &#34;
                f&#34;{cls.CACHE_MULTIPROCESSING_CONTEXT}. &#34;
                &#34;Trying to switch it. However &#34;
                f&#34;{cls.CACHE_MULTIPROCESSING_CONTEXT} may be unsafe or &#34;
                &#34;unsupported. The most safe option is to launch your process &#34;
                &#34;with fixed `PYTHONHASHSEED` env and remain `spawn` as start &#34;
                &#34;method.\n&#34;
                &#34;Possible launch is `PYTHONHASHSEED=0 python3 run.py&#34;
            )

        if cls.CACHE_MULTIPROCESSING_CONTEXT not in mp.get_all_start_methods():
            raise OSError(
                f&#34;Cache can&#39;t be used without setting `PYTHONHASHSEED`. &#34;
                f&#34;{cls.CACHE_MULTIPROCESSING_CONTEXT} multiprocessing context &#34;
                &#34;is not available on current OS&#34;
            )</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="quaterion.train.trainable_model.TrainableModel" href="trainable_model.html#quaterion.train.trainable_model.TrainableModel">TrainableModel</a></li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="quaterion.train.cache_mixin.CacheMixin.CACHE_MULTIPROCESSING_CONTEXT"><code class="name">var <span class="ident">CACHE_MULTIPROCESSING_CONTEXT</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Static methods</h3>
<dl>
<dt id="quaterion.train.cache_mixin.CacheMixin.cache"><code class="name flex">
<span>def <span class="ident">cache</span></span>(<span>trainer: pytorch_lightning.trainer.trainer.Trainer, encoders: Dict[str, quaterion_models.encoders.encoder.Encoder], train_dataloader: <a title="quaterion.dataset.similarity_data_loader.SimilarityDataLoader" href="../dataset/similarity_data_loader.html#quaterion.dataset.similarity_data_loader.SimilarityDataLoader">SimilarityDataLoader</a>, val_dataloader: Optional[<a title="quaterion.dataset.similarity_data_loader.SimilarityDataLoader" href="../dataset/similarity_data_loader.html#quaterion.dataset.similarity_data_loader.SimilarityDataLoader">SimilarityDataLoader</a>], cache_config: <a title="quaterion.train.encoders.cache_config.CacheConfig" href="encoders/cache_config.html#quaterion.train.encoders.cache_config.CacheConfig">CacheConfig</a>) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Filling cache for model's cache encoders.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>trainer</code></strong></dt>
<dd>performs one training epoch to cache encoders outputs.
Preserve all encapsulated pytorch-lightning logic such as
device managing etc.</dd>
<dt><strong><code>encoders</code></strong></dt>
<dd>mapping of model's encoders and their names</dd>
<dt><strong><code>train_dataloader</code></strong></dt>
<dd>model's train dataloader</dd>
<dt><strong><code>val_dataloader</code></strong></dt>
<dd>model's val dataloader</dd>
<dt><strong><code>cache_config</code></strong></dt>
<dd>cache config instance to configure cache batch size
and num of workers to use for caching</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def cache(
    cls,
    trainer: pl.Trainer,
    encoders: Dict[str, Encoder],
    train_dataloader: SimilarityDataLoader,
    val_dataloader: Optional[SimilarityDataLoader],
    cache_config: CacheConfig,
) -&gt; None:
    &#34;&#34;&#34;Filling cache for model&#39;s cache encoders.

    Args:
        trainer: performs one training epoch to cache encoders outputs.
            Preserve all encapsulated pytorch-lightning logic such as
            device managing etc.
        encoders: mapping of model&#39;s encoders and their names
        train_dataloader: model&#39;s train dataloader
        val_dataloader: model&#39;s val dataloader
        cache_config: cache config instance to configure cache batch size
            and num of workers to use for caching

    &#34;&#34;&#34;
    cache_encoders = {
        name: encoder
        for name, encoder in encoders.items()
        if isinstance(encoder, CacheEncoder)
    }

    if not cache_encoders:
        return

    cls._compatibility_check(train_dataloader)
    train_dataloader = cls._wrap_cache_dataloader(
        train_dataloader, cache_config, cache_encoders
    )

    if val_dataloader is not None:
        val_dataloader = cls._wrap_cache_dataloader(
            val_dataloader, cache_config, cache_encoders
        )

    cls._fill_cache(trainer, cache_encoders, train_dataloader, val_dataloader)

    # Once cache is filled, collate functions return only keys for cache
    for encoder in cache_encoders.values():
        encoder.cache_filled = True
    logger.info(&#34;Caching has been successfully finished&#34;)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="quaterion.train.cache_mixin.CacheModel"><code class="flex name class">
<span>class <span class="ident">CacheModel</span></span>
<span>(</span><span>encoders: Dict[str, <a title="quaterion.train.encoders.cache_encoder.CacheEncoder" href="encoders/cache_encoder.html#quaterion.train.encoders.cache_encoder.CacheEncoder">CacheEncoder</a>])</span>
</code></dt>
<dd>
<div class="desc"><p>Mock model for convenient caching.</p>
<p>This class is required to make caching process similar to the training of
the genuine model and inherit and use the same trainer instance. It allows
avoiding of messing with device managing stuff and more.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>encoders</code></strong></dt>
<dd>dict of cache encoders names and corresponding instances to cache</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CacheModel(pl.LightningModule):
    &#34;&#34;&#34;Mock model for convenient caching.

    This class is required to make caching process similar to the training of
    the genuine model and inherit and use the same trainer instance. It allows
    avoiding of messing with device managing stuff and more.

    Args:
        encoders: dict of cache encoders names and corresponding instances to cache
    &#34;&#34;&#34;

    def __init__(
        self,
        encoders: Dict[str, CacheEncoder],
    ):

        super().__init__()
        self.encoders = encoders
        for key, encoder in self.encoders.items():
            self.add_module(key, encoder)

    def predict_step(
        self,
        batch: Dict[str, Tuple[Iterable[Hashable], TensorInterchange]],
        batch_idx: int,
        dataloader_idx: Optional[int] = None,
    ):
        &#34;&#34;&#34;Caches batch of input.

        Args:
            batch: batch of collated data. Contains mapping, where key is
                encoder&#39;s name, value is tuple of key used in cache and
                according items, which are ready to be processed by specific
                encoder.
            batch_idx: Index of current batch
            dataloader_idx: Index of the current dataloader
        Returns:
            torch.Tensor: loss mock
        &#34;&#34;&#34;
        for encoder_name, encoder in self.encoders.items():
            encoder_samples = batch.get(encoder_name)
            if not encoder_samples:
                continue
            encoder.fill_cache(encoder_samples)

        return torch.Tensor([1])

    # region anchors
    def train_dataloader(self) -&gt; TRAIN_DATALOADERS:
        pass

    def test_dataloader(self) -&gt; EVAL_DATALOADERS:
        pass

    def val_dataloader(self) -&gt; EVAL_DATALOADERS:
        pass

    def predict_dataloader(self) -&gt; EVAL_DATALOADERS:
        pass

    # endregion</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>pytorch_lightning.core.lightning.LightningModule</li>
<li>pytorch_lightning.core.mixins.device_dtype_mixin.DeviceDtypeModuleMixin</li>
<li>pytorch_lightning.core.mixins.hparams_mixin.HyperparametersMixin</li>
<li>pytorch_lightning.core.saving.ModelIO</li>
<li>pytorch_lightning.core.hooks.ModelHooks</li>
<li>pytorch_lightning.core.hooks.DataHooks</li>
<li>pytorch_lightning.core.hooks.CheckpointHooks</li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="quaterion.train.cache_mixin.CacheModel.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="quaterion.train.cache_mixin.CacheModel.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="quaterion.train.cache_mixin.CacheModel.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, *args, **kwargs) ‑> Any</span>
</code></dt>
<dd>
<div class="desc"><p>Same as :meth:<code>torch.nn.Module.forward()</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>*args</code></strong></dt>
<dd>Whatever you decide to pass into the forward method.</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Keyword arguments are also possible.</dd>
</dl>
<h2 id="return">Return</h2>
<p>Your model's output</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, *args, **kwargs) -&gt; Any:
    r&#34;&#34;&#34;
    Same as :meth:`torch.nn.Module.forward()`.

    Args:
        *args: Whatever you decide to pass into the forward method.
        **kwargs: Keyword arguments are also possible.

    Return:
        Your model&#39;s output
    &#34;&#34;&#34;
    return super().forward(*args, **kwargs)</code></pre>
</details>
</dd>
<dt id="quaterion.train.cache_mixin.CacheModel.predict_dataloader"><code class="name flex">
<span>def <span class="ident">predict_dataloader</span></span>(<span>self) ‑> Union[torch.utils.data.dataloader.DataLoader, Sequence[torch.utils.data.dataloader.DataLoader]]</span>
</code></dt>
<dd>
<div class="desc"><p>Implement one or multiple PyTorch DataLoaders for prediction.</p>
<p>It's recommended that all data downloads and preparation happen in :meth:<code>prepare_data</code>.</p>
<ul>
<li>:meth:<code>~pytorch_lightning.trainer.Trainer.fit</code></li>
<li>&hellip;</li>
<li>:meth:<code>prepare_data</code></li>
<li>:meth:<code>train_dataloader</code></li>
<li>:meth:<code>val_dataloader</code></li>
<li>:meth:<code>test_dataloader</code></li>
</ul>
<h2 id="note">Note</h2>
<p>Lightning adds the correct sampler for distributed and arbitrary hardware
There is no need to set it yourself.</p>
<h2 id="return">Return</h2>
<p>A :class:<code>torch.utils.data.DataLoader</code> or a sequence of them specifying prediction samples.</p>
<h2 id="note_1">Note</h2>
<p>In the case where you return multiple prediction dataloaders, the :meth:<code>predict</code>
will have an argument <code>dataloader_idx</code> which matches the order here.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict_dataloader(self) -&gt; EVAL_DATALOADERS:
    pass</code></pre>
</details>
</dd>
<dt id="quaterion.train.cache_mixin.CacheModel.predict_step"><code class="name flex">
<span>def <span class="ident">predict_step</span></span>(<span>self, batch: Dict[str, Tuple[Iterable[Hashable], Union[torch.Tensor, Tuple[torch.Tensor], List[torch.Tensor], Dict[str, torch.Tensor], Dict[str, dict], Any]]], batch_idx: int, dataloader_idx: Optional[int] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Caches batch of input.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>batch</code></strong></dt>
<dd>batch of collated data. Contains mapping, where key is
encoder's name, value is tuple of key used in cache and
according items, which are ready to be processed by specific
encoder.</dd>
<dt><strong><code>batch_idx</code></strong></dt>
<dd>Index of current batch</dd>
<dt><strong><code>dataloader_idx</code></strong></dt>
<dd>Index of the current dataloader</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>torch.Tensor</code></dt>
<dd>loss mock</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict_step(
    self,
    batch: Dict[str, Tuple[Iterable[Hashable], TensorInterchange]],
    batch_idx: int,
    dataloader_idx: Optional[int] = None,
):
    &#34;&#34;&#34;Caches batch of input.

    Args:
        batch: batch of collated data. Contains mapping, where key is
            encoder&#39;s name, value is tuple of key used in cache and
            according items, which are ready to be processed by specific
            encoder.
        batch_idx: Index of current batch
        dataloader_idx: Index of the current dataloader
    Returns:
        torch.Tensor: loss mock
    &#34;&#34;&#34;
    for encoder_name, encoder in self.encoders.items():
        encoder_samples = batch.get(encoder_name)
        if not encoder_samples:
            continue
        encoder.fill_cache(encoder_samples)

    return torch.Tensor([1])</code></pre>
</details>
</dd>
<dt id="quaterion.train.cache_mixin.CacheModel.test_dataloader"><code class="name flex">
<span>def <span class="ident">test_dataloader</span></span>(<span>self) ‑> Union[torch.utils.data.dataloader.DataLoader, Sequence[torch.utils.data.dataloader.DataLoader]]</span>
</code></dt>
<dd>
<div class="desc"><p>Implement one or multiple PyTorch DataLoaders for testing.</p>
<p>The dataloader you return will not be reloaded unless you set
:paramref:<code>~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_n_epochs</code> to
a postive integer.</p>
<p>For data processing use the following pattern:</p>
<pre><code>- download in :meth:&lt;code&gt;prepare\_data&lt;/code&gt;
- process and split in :meth:&lt;code&gt;setup&lt;/code&gt;
</code></pre>
<p>However, the above are only necessary for distributed processing.</p>
<div class="admonition warning">
<p class="admonition-title">Warning:&ensp;do not assign state in prepare_data</p>
</div>
<ul>
<li>:meth:<code>~pytorch_lightning.trainer.Trainer.fit</code></li>
<li>&hellip;</li>
<li>:meth:<code>prepare_data</code></li>
<li>:meth:<code>setup</code></li>
<li>:meth:<code>train_dataloader</code></li>
<li>:meth:<code>val_dataloader</code></li>
<li>:meth:<code>test_dataloader</code></li>
</ul>
<h2 id="note">Note</h2>
<p>Lightning adds the correct sampler for distributed and arbitrary hardware.
There is no need to set it yourself.</p>
<h2 id="return">Return</h2>
<p>A :class:<code>torch.utils.data.DataLoader</code> or a sequence of them specifying testing samples.</p>
<p>Example::</p>
<pre><code>def test_dataloader(self):
    transform = transforms.Compose([transforms.ToTensor(),
                                    transforms.Normalize((0.5,), (1.0,))])
    dataset = MNIST(root='/path/to/mnist/', train=False, transform=transform,
                    download=True)
    loader = torch.utils.data.DataLoader(
        dataset=dataset,
        batch_size=self.batch_size,
        shuffle=False
    )

    return loader

# can also return multiple dataloaders
def test_dataloader(self):
    return [loader_a, loader_b, ..., loader_n]
</code></pre>
<h2 id="note_1">Note</h2>
<p>If you don't need a test dataset and a :meth:<code>test_step</code>, you don't need to implement
this method.</p>
<h2 id="note_2">Note</h2>
<p>In the case where you return multiple test dataloaders, the :meth:<code>test_step</code>
will have an argument <code>dataloader_idx</code> which matches the order here.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_dataloader(self) -&gt; EVAL_DATALOADERS:
    pass</code></pre>
</details>
</dd>
<dt id="quaterion.train.cache_mixin.CacheModel.train_dataloader"><code class="name flex">
<span>def <span class="ident">train_dataloader</span></span>(<span>self) ‑> Union[torch.utils.data.dataloader.DataLoader, Sequence[torch.utils.data.dataloader.DataLoader], Sequence[Sequence[torch.utils.data.dataloader.DataLoader]], Sequence[Dict[str, torch.utils.data.dataloader.DataLoader]], Dict[str, torch.utils.data.dataloader.DataLoader], Dict[str, Dict[str, torch.utils.data.dataloader.DataLoader]], Dict[str, Sequence[torch.utils.data.dataloader.DataLoader]]]</span>
</code></dt>
<dd>
<div class="desc"><p>Implement one or more PyTorch DataLoaders for training.</p>
<h2 id="return">Return</h2>
<p>A collection of :class:<code>torch.utils.data.DataLoader</code> specifying training samples.
In the case of multiple dataloaders, please see this :ref:<code>page &lt;multiple-training-dataloaders&gt;</code>.</p>
<p>The dataloader you return will not be reloaded unless you set
:paramref:<code>~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_n_epochs</code> to
a positive integer.</p>
<p>For data processing use the following pattern:</p>
<pre><code>- download in :meth:&lt;code&gt;prepare\_data&lt;/code&gt;
- process and split in :meth:&lt;code&gt;setup&lt;/code&gt;
</code></pre>
<p>However, the above are only necessary for distributed processing.</p>
<div class="admonition warning">
<p class="admonition-title">Warning:&ensp;do not assign state in prepare_data</p>
</div>
<ul>
<li>:meth:<code>~pytorch_lightning.trainer.Trainer.fit</code></li>
<li>&hellip;</li>
<li>:meth:<code>prepare_data</code></li>
<li>:meth:<code>setup</code></li>
<li>:meth:<code>train_dataloader</code></li>
</ul>
<h2 id="note">Note</h2>
<p>Lightning adds the correct sampler for distributed and arbitrary hardware.
There is no need to set it yourself.</p>
<p>Example::</p>
<pre><code># single dataloader
def train_dataloader(self):
    transform = transforms.Compose([transforms.ToTensor(),
                                    transforms.Normalize((0.5,), (1.0,))])
    dataset = MNIST(root='/path/to/mnist/', train=True, transform=transform,
                    download=True)
    loader = torch.utils.data.DataLoader(
        dataset=dataset,
        batch_size=self.batch_size,
        shuffle=True
    )
    return loader

# multiple dataloaders, return as list
def train_dataloader(self):
    mnist = MNIST(...)
    cifar = CIFAR(...)
    mnist_loader = torch.utils.data.DataLoader(
        dataset=mnist, batch_size=self.batch_size, shuffle=True
    )
    cifar_loader = torch.utils.data.DataLoader(
        dataset=cifar, batch_size=self.batch_size, shuffle=True
    )
    # each batch will be a list of tensors: [batch_mnist, batch_cifar]
    return [mnist_loader, cifar_loader]

# multiple dataloader, return as dict
def train_dataloader(self):
    mnist = MNIST(...)
    cifar = CIFAR(...)
    mnist_loader = torch.utils.data.DataLoader(
        dataset=mnist, batch_size=self.batch_size, shuffle=True
    )
    cifar_loader = torch.utils.data.DataLoader(
        dataset=cifar, batch_size=self.batch_size, shuffle=True
    )
    # each batch will be a dict of tensors: {'mnist': batch_mnist, 'cifar': batch_cifar}
    return {'mnist': mnist_loader, 'cifar': cifar_loader}
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train_dataloader(self) -&gt; TRAIN_DATALOADERS:
    pass</code></pre>
</details>
</dd>
<dt id="quaterion.train.cache_mixin.CacheModel.val_dataloader"><code class="name flex">
<span>def <span class="ident">val_dataloader</span></span>(<span>self) ‑> Union[torch.utils.data.dataloader.DataLoader, Sequence[torch.utils.data.dataloader.DataLoader]]</span>
</code></dt>
<dd>
<div class="desc"><p>Implement one or multiple PyTorch DataLoaders for validation.</p>
<p>The dataloader you return will not be reloaded unless you set
:paramref:<code>~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_n_epochs</code> to
a positive integer.</p>
<p>It's recommended that all data downloads and preparation happen in :meth:<code>prepare_data</code>.</p>
<ul>
<li>:meth:<code>~pytorch_lightning.trainer.Trainer.fit</code></li>
<li>&hellip;</li>
<li>:meth:<code>prepare_data</code></li>
<li>:meth:<code>train_dataloader</code></li>
<li>:meth:<code>val_dataloader</code></li>
<li>:meth:<code>test_dataloader</code></li>
</ul>
<h2 id="note">Note</h2>
<p>Lightning adds the correct sampler for distributed and arbitrary hardware
There is no need to set it yourself.</p>
<h2 id="return">Return</h2>
<p>A :class:<code>torch.utils.data.DataLoader</code> or a sequence of them specifying validation samples.</p>
<p>Examples::</p>
<pre><code>def val_dataloader(self):
    transform = transforms.Compose([transforms.ToTensor(),
                                    transforms.Normalize((0.5,), (1.0,))])
    dataset = MNIST(root='/path/to/mnist/', train=False,
                    transform=transform, download=True)
    loader = torch.utils.data.DataLoader(
        dataset=dataset,
        batch_size=self.batch_size,
        shuffle=False
    )

    return loader

# can also return multiple dataloaders
def val_dataloader(self):
    return [loader_a, loader_b, ..., loader_n]
</code></pre>
<h2 id="note_1">Note</h2>
<p>If you don't need a validation dataset and a :meth:<code>validation_step</code>, you don't need to
implement this method.</p>
<h2 id="note_2">Note</h2>
<p>In the case where you return multiple validation dataloaders, the :meth:<code>validation_step</code>
will have an argument <code>dataloader_idx</code> which matches the order here.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def val_dataloader(self) -&gt; EVAL_DATALOADERS:
    pass</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="quaterion.train" href="index.html">quaterion.train</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="quaterion.train.cache_mixin.CacheMixin" href="#quaterion.train.cache_mixin.CacheMixin">CacheMixin</a></code></h4>
<ul class="">
<li><code><a title="quaterion.train.cache_mixin.CacheMixin.CACHE_MULTIPROCESSING_CONTEXT" href="#quaterion.train.cache_mixin.CacheMixin.CACHE_MULTIPROCESSING_CONTEXT">CACHE_MULTIPROCESSING_CONTEXT</a></code></li>
<li><code><a title="quaterion.train.cache_mixin.CacheMixin.cache" href="#quaterion.train.cache_mixin.CacheMixin.cache">cache</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="quaterion.train.cache_mixin.CacheModel" href="#quaterion.train.cache_mixin.CacheModel">CacheModel</a></code></h4>
<ul class="two-column">
<li><code><a title="quaterion.train.cache_mixin.CacheModel.dump_patches" href="#quaterion.train.cache_mixin.CacheModel.dump_patches">dump_patches</a></code></li>
<li><code><a title="quaterion.train.cache_mixin.CacheModel.forward" href="#quaterion.train.cache_mixin.CacheModel.forward">forward</a></code></li>
<li><code><a title="quaterion.train.cache_mixin.CacheModel.predict_dataloader" href="#quaterion.train.cache_mixin.CacheModel.predict_dataloader">predict_dataloader</a></code></li>
<li><code><a title="quaterion.train.cache_mixin.CacheModel.predict_step" href="#quaterion.train.cache_mixin.CacheModel.predict_step">predict_step</a></code></li>
<li><code><a title="quaterion.train.cache_mixin.CacheModel.test_dataloader" href="#quaterion.train.cache_mixin.CacheModel.test_dataloader">test_dataloader</a></code></li>
<li><code><a title="quaterion.train.cache_mixin.CacheModel.train_dataloader" href="#quaterion.train.cache_mixin.CacheModel.train_dataloader">train_dataloader</a></code></li>
<li><code><a title="quaterion.train.cache_mixin.CacheModel.training" href="#quaterion.train.cache_mixin.CacheModel.training">training</a></code></li>
<li><code><a title="quaterion.train.cache_mixin.CacheModel.val_dataloader" href="#quaterion.train.cache_mixin.CacheModel.val_dataloader">val_dataloader</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>