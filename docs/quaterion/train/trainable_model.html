<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>quaterion.train.trainable_model API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>quaterion.train.trainable_model</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from typing import Dict, Any, Union, Optional

import pytorch_lightning as pl

from torch import Tensor
from pytorch_lightning.utilities.types import (
    TRAIN_DATALOADERS,
    EVAL_DATALOADERS,
)

from quaterion_models.encoders import Encoder
from quaterion_models.heads import EncoderHead
from quaterion_models import MetricModel
from quaterion_models.types import TensorInterchange
from quaterion.train.encoders import (
    CacheConfig,
    CacheType,
)
from quaterion.loss import SimilarityLoss
from quaterion.utils.enums import TrainStage
from quaterion.train.cache_mixin import CacheMixin


class TrainableModel(pl.LightningModule, CacheMixin):
    &#34;&#34;&#34;Base class for models to be trained.&#34;&#34;&#34;

    def __init__(self, *args: Any, **kwargs: Any):
        super().__init__(*args, **kwargs)

        encoders = self.configure_encoders()
        self.cache_config = self.configure_caches()
        encoders = self._apply_cache_config(encoders, self.cache_config)

        head = self.configure_head(MetricModel.get_encoders_output_size(encoders))

        self._model = MetricModel(encoders=encoders, head=head)
        self._loss = self.configure_loss()

    @property
    def model(self) -&gt; MetricModel:
        &#34;&#34;&#34;Origin model to be trained

        Returns:
            MetricModel: model to be trained
        &#34;&#34;&#34;
        return self._model

    @property
    def loss(self) -&gt; SimilarityLoss:
        &#34;&#34;&#34;Property to get the loss function to use.&#34;&#34;&#34;
        return self._loss

    def configure_loss(self) -&gt; SimilarityLoss:
        &#34;&#34;&#34;Method to configure loss function to use.&#34;&#34;&#34;
        raise NotImplementedError()

    def configure_encoders(self) -&gt; Union[Encoder, Dict[str, Encoder]]:
        &#34;&#34;&#34;Method to provide encoders configuration

        Use this function to define an initial state of encoders.
        This function should be used to assign initial values for encoders
        before training as well as during the checkpoint loading.

        Returns:
             Union[Encoder, Dict[str, Encoder]]: one instance of encoder which will
                have quaterion_models.model.DEFAULT_ENCODER_KEY, or dict of encoder
                names and corresponding encoder instances.
        &#34;&#34;&#34;
        raise NotImplementedError()

    def configure_caches(self) -&gt; Optional[CacheConfig]:
        &#34;&#34;&#34;Method to provide cache configuration

                Use this method to define which encoders should cache calculated embeddings and
                what kind of cache they should use.

                Returns:
                    Optional[CacheConfig]: cache configuration to be applied if provided, None
                        otherwise
                Examples:

                &gt;&gt;&gt; CacheConfig(CacheType.AUTO)
                CacheConfig(
                    cache_type=&lt;CacheType.AUTO: &#39;auto&#39;&gt;,
                    mapping={},
                    key_extractors={}
                )

                &gt;&gt;&gt; cache_config = CacheConfig(
        ...     mapping={&#34;text_encoder&#34;: CacheType.GPU, &#34;image_encoder&#34;: CacheType.CPU}
        ... )
                CacheConfig(
                    cache_type=None,
                    mapping={
                        &#39;text_encoder&#39;: &lt;CacheType.GPU: &#39;gpu&#39;&gt;,
                        &#39;image_encoder&#39;: &lt;CacheType.CPU: &#39;cpu&#39;&gt;
                    },
                    key_extractors={}
                )
                &gt;&gt;&gt; CacheConfig(
        ...     cache_type=CacheType.AUTO,
        ...     key_extractors={&#34;default&#34;: lambda obj: hash(obj)}
        ... )
                CacheConfig(
                    cache_type=&lt;CacheType.AUTO: &#39;auto&#39;&gt;,
                    mapping={},
                    key_extractors={&#39;default&#39;: &lt;function &lt;lambda&gt; at 0x106bc90e0&gt;}
                )

        &#34;&#34;&#34;
        pass

    def configure_head(self, input_embedding_size: int) -&gt; EncoderHead:
        &#34;&#34;&#34;Use this function to define an initial state for head layer of the model.

        Args:
            input_embedding_size: size of embeddings produced by encoders
        Returns:
            EncoderHead: instance of EncoderHead to be added to the model
        &#34;&#34;&#34;
        raise NotImplementedError()

    def process_results(
        self,
        embeddings: Tensor,
        targets: Dict[str, Any],
        batch_idx: int,
        stage: TrainStage,
        **kwargs,
    ):
        &#34;&#34;&#34;Method to provide any additional evaluations of embeddings.

        Args:
            embeddings: shape: (batch_size, embedding_size) - model&#39;s output.
            targets: output of batch target collate.
            batch_idx: ID of the processing batch.
            stage: train, validation or test stage.
        &#34;&#34;&#34;
        pass

    def training_step(
        self, batch: TensorInterchange, batch_idx: int, **kwargs
    ) -&gt; Tensor:
        &#34;&#34;&#34;Compute and return the training loss and some additional metrics for e.g.
        the progress bar or logger.

        Args:
            batch: Output of DataLoader.
            batch_idx: Integer displaying index of this batch.
            **kwargs: keyword arguments to be passed into `process_results`

        Returns:
            Tensor: computed loss value
        &#34;&#34;&#34;
        stage = TrainStage.TRAIN
        loss = self._common_step(
            batch=batch, batch_idx=batch_idx, stage=stage, **kwargs
        )
        return loss

    def validation_step(self, batch, batch_idx, **kwargs) -&gt; Optional[Tensor]:
        &#34;&#34;&#34;Compute validation loss and some additional metrics for e.g. the progress
        bar or logger.

        Args:
            batch: Output of DataLoader.
            batch_idx: Integer displaying index of this batch.
            **kwargs: keyword arguments to be passed into `process_results`
        &#34;&#34;&#34;
        stage = TrainStage.VALIDATION
        self._common_step(batch=batch, batch_idx=batch_idx, stage=stage, **kwargs)
        return None

    def test_step(self, batch, batch_idx, **kwargs) -&gt; Optional[Tensor]:
        &#34;&#34;&#34;Compute test loss and some additional metrics for e.g. the progress
        bar or logger.

        Args:
            batch: Output of DataLoader.
            batch_idx: Integer displaying index of this batch.
            **kwargs: keyword arguments to be passed into `process_results`
        &#34;&#34;&#34;
        stage = TrainStage.TEST
        self._common_step(batch=batch, batch_idx=batch_idx, stage=stage, **kwargs)
        return None

    def _common_step(self, batch, batch_idx, stage: TrainStage, **kwargs) -&gt; Tensor:
        &#34;&#34;&#34;Common step to compute loss and metrics for training, validation, test
         and other stages.

        Args:
            batch: Output of DataLoader.
            batch_idx: Integer displaying index of this batch.
            stage: current training stage: training, validation, etc.
            **kwargs: keyword arguments to be passed into `process_results`

        Returns:
            Tensor: computed loss value
        &#34;&#34;&#34;
        features, targets = batch
        embeddings = self.model(features)
        loss = self.loss(embeddings=embeddings, **targets)
        self.log(f&#34;{stage}_loss&#34;, loss)
        self.process_results(
            embeddings=embeddings,
            targets=targets,
            batch_idx=batch_idx,
            stage=stage,
            **kwargs,
        )
        return loss

    def save_servable(self, path: str):
        &#34;&#34;&#34;Save model for serving, independent of Pytorch Lightning

        Args:
            path: path to save to
        &#34;&#34;&#34;
        self.model.save(path)

    # region anchors
    # https://github.com/PyTorchLightning/pytorch-lightning/issues/10667
    def train_dataloader(self) -&gt; TRAIN_DATALOADERS:
        pass

    def test_dataloader(self) -&gt; EVAL_DATALOADERS:
        pass

    def val_dataloader(self) -&gt; EVAL_DATALOADERS:
        pass

    def predict_dataloader(self) -&gt; EVAL_DATALOADERS:
        pass

    # endregion</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="quaterion.train.trainable_model.TrainableModel"><code class="flex name class">
<span>class <span class="ident">TrainableModel</span></span>
<span>(</span><span>*args: Any, **kwargs: Any)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for models to be trained.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TrainableModel(pl.LightningModule, CacheMixin):
    &#34;&#34;&#34;Base class for models to be trained.&#34;&#34;&#34;

    def __init__(self, *args: Any, **kwargs: Any):
        super().__init__(*args, **kwargs)

        encoders = self.configure_encoders()
        self.cache_config = self.configure_caches()
        encoders = self._apply_cache_config(encoders, self.cache_config)

        head = self.configure_head(MetricModel.get_encoders_output_size(encoders))

        self._model = MetricModel(encoders=encoders, head=head)
        self._loss = self.configure_loss()

    @property
    def model(self) -&gt; MetricModel:
        &#34;&#34;&#34;Origin model to be trained

        Returns:
            MetricModel: model to be trained
        &#34;&#34;&#34;
        return self._model

    @property
    def loss(self) -&gt; SimilarityLoss:
        &#34;&#34;&#34;Property to get the loss function to use.&#34;&#34;&#34;
        return self._loss

    def configure_loss(self) -&gt; SimilarityLoss:
        &#34;&#34;&#34;Method to configure loss function to use.&#34;&#34;&#34;
        raise NotImplementedError()

    def configure_encoders(self) -&gt; Union[Encoder, Dict[str, Encoder]]:
        &#34;&#34;&#34;Method to provide encoders configuration

        Use this function to define an initial state of encoders.
        This function should be used to assign initial values for encoders
        before training as well as during the checkpoint loading.

        Returns:
             Union[Encoder, Dict[str, Encoder]]: one instance of encoder which will
                have quaterion_models.model.DEFAULT_ENCODER_KEY, or dict of encoder
                names and corresponding encoder instances.
        &#34;&#34;&#34;
        raise NotImplementedError()

    def configure_caches(self) -&gt; Optional[CacheConfig]:
        &#34;&#34;&#34;Method to provide cache configuration

                Use this method to define which encoders should cache calculated embeddings and
                what kind of cache they should use.

                Returns:
                    Optional[CacheConfig]: cache configuration to be applied if provided, None
                        otherwise
                Examples:

                &gt;&gt;&gt; CacheConfig(CacheType.AUTO)
                CacheConfig(
                    cache_type=&lt;CacheType.AUTO: &#39;auto&#39;&gt;,
                    mapping={},
                    key_extractors={}
                )

                &gt;&gt;&gt; cache_config = CacheConfig(
        ...     mapping={&#34;text_encoder&#34;: CacheType.GPU, &#34;image_encoder&#34;: CacheType.CPU}
        ... )
                CacheConfig(
                    cache_type=None,
                    mapping={
                        &#39;text_encoder&#39;: &lt;CacheType.GPU: &#39;gpu&#39;&gt;,
                        &#39;image_encoder&#39;: &lt;CacheType.CPU: &#39;cpu&#39;&gt;
                    },
                    key_extractors={}
                )
                &gt;&gt;&gt; CacheConfig(
        ...     cache_type=CacheType.AUTO,
        ...     key_extractors={&#34;default&#34;: lambda obj: hash(obj)}
        ... )
                CacheConfig(
                    cache_type=&lt;CacheType.AUTO: &#39;auto&#39;&gt;,
                    mapping={},
                    key_extractors={&#39;default&#39;: &lt;function &lt;lambda&gt; at 0x106bc90e0&gt;}
                )

        &#34;&#34;&#34;
        pass

    def configure_head(self, input_embedding_size: int) -&gt; EncoderHead:
        &#34;&#34;&#34;Use this function to define an initial state for head layer of the model.

        Args:
            input_embedding_size: size of embeddings produced by encoders
        Returns:
            EncoderHead: instance of EncoderHead to be added to the model
        &#34;&#34;&#34;
        raise NotImplementedError()

    def process_results(
        self,
        embeddings: Tensor,
        targets: Dict[str, Any],
        batch_idx: int,
        stage: TrainStage,
        **kwargs,
    ):
        &#34;&#34;&#34;Method to provide any additional evaluations of embeddings.

        Args:
            embeddings: shape: (batch_size, embedding_size) - model&#39;s output.
            targets: output of batch target collate.
            batch_idx: ID of the processing batch.
            stage: train, validation or test stage.
        &#34;&#34;&#34;
        pass

    def training_step(
        self, batch: TensorInterchange, batch_idx: int, **kwargs
    ) -&gt; Tensor:
        &#34;&#34;&#34;Compute and return the training loss and some additional metrics for e.g.
        the progress bar or logger.

        Args:
            batch: Output of DataLoader.
            batch_idx: Integer displaying index of this batch.
            **kwargs: keyword arguments to be passed into `process_results`

        Returns:
            Tensor: computed loss value
        &#34;&#34;&#34;
        stage = TrainStage.TRAIN
        loss = self._common_step(
            batch=batch, batch_idx=batch_idx, stage=stage, **kwargs
        )
        return loss

    def validation_step(self, batch, batch_idx, **kwargs) -&gt; Optional[Tensor]:
        &#34;&#34;&#34;Compute validation loss and some additional metrics for e.g. the progress
        bar or logger.

        Args:
            batch: Output of DataLoader.
            batch_idx: Integer displaying index of this batch.
            **kwargs: keyword arguments to be passed into `process_results`
        &#34;&#34;&#34;
        stage = TrainStage.VALIDATION
        self._common_step(batch=batch, batch_idx=batch_idx, stage=stage, **kwargs)
        return None

    def test_step(self, batch, batch_idx, **kwargs) -&gt; Optional[Tensor]:
        &#34;&#34;&#34;Compute test loss and some additional metrics for e.g. the progress
        bar or logger.

        Args:
            batch: Output of DataLoader.
            batch_idx: Integer displaying index of this batch.
            **kwargs: keyword arguments to be passed into `process_results`
        &#34;&#34;&#34;
        stage = TrainStage.TEST
        self._common_step(batch=batch, batch_idx=batch_idx, stage=stage, **kwargs)
        return None

    def _common_step(self, batch, batch_idx, stage: TrainStage, **kwargs) -&gt; Tensor:
        &#34;&#34;&#34;Common step to compute loss and metrics for training, validation, test
         and other stages.

        Args:
            batch: Output of DataLoader.
            batch_idx: Integer displaying index of this batch.
            stage: current training stage: training, validation, etc.
            **kwargs: keyword arguments to be passed into `process_results`

        Returns:
            Tensor: computed loss value
        &#34;&#34;&#34;
        features, targets = batch
        embeddings = self.model(features)
        loss = self.loss(embeddings=embeddings, **targets)
        self.log(f&#34;{stage}_loss&#34;, loss)
        self.process_results(
            embeddings=embeddings,
            targets=targets,
            batch_idx=batch_idx,
            stage=stage,
            **kwargs,
        )
        return loss

    def save_servable(self, path: str):
        &#34;&#34;&#34;Save model for serving, independent of Pytorch Lightning

        Args:
            path: path to save to
        &#34;&#34;&#34;
        self.model.save(path)

    # region anchors
    # https://github.com/PyTorchLightning/pytorch-lightning/issues/10667
    def train_dataloader(self) -&gt; TRAIN_DATALOADERS:
        pass

    def test_dataloader(self) -&gt; EVAL_DATALOADERS:
        pass

    def val_dataloader(self) -&gt; EVAL_DATALOADERS:
        pass

    def predict_dataloader(self) -&gt; EVAL_DATALOADERS:
        pass

    # endregion</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>pytorch_lightning.core.lightning.LightningModule</li>
<li>pytorch_lightning.core.mixins.device_dtype_mixin.DeviceDtypeModuleMixin</li>
<li>pytorch_lightning.core.mixins.hparams_mixin.HyperparametersMixin</li>
<li>pytorch_lightning.core.saving.ModelIO</li>
<li>pytorch_lightning.core.hooks.ModelHooks</li>
<li>pytorch_lightning.core.hooks.DataHooks</li>
<li>pytorch_lightning.core.hooks.CheckpointHooks</li>
<li>torch.nn.modules.module.Module</li>
<li><a title="quaterion.train.cache_mixin.CacheMixin" href="cache_mixin.html#quaterion.train.cache_mixin.CacheMixin">CacheMixin</a></li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="quaterion.train.trainable_model.TrainableModel.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="quaterion.train.trainable_model.TrainableModel.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="quaterion.train.trainable_model.TrainableModel.loss"><code class="name">var <span class="ident">loss</span> : <a title="quaterion.loss.similarity_loss.SimilarityLoss" href="../loss/similarity_loss.html#quaterion.loss.similarity_loss.SimilarityLoss">SimilarityLoss</a></code></dt>
<dd>
<div class="desc"><p>Property to get the loss function to use.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def loss(self) -&gt; SimilarityLoss:
    &#34;&#34;&#34;Property to get the loss function to use.&#34;&#34;&#34;
    return self._loss</code></pre>
</details>
</dd>
<dt id="quaterion.train.trainable_model.TrainableModel.model"><code class="name">var <span class="ident">model</span> : quaterion_models.model.MetricModel</code></dt>
<dd>
<div class="desc"><p>Origin model to be trained</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>MetricModel</code></dt>
<dd>model to be trained</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def model(self) -&gt; MetricModel:
    &#34;&#34;&#34;Origin model to be trained

    Returns:
        MetricModel: model to be trained
    &#34;&#34;&#34;
    return self._model</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="quaterion.train.trainable_model.TrainableModel.configure_caches"><code class="name flex">
<span>def <span class="ident">configure_caches</span></span>(<span>self) ‑> Optional[<a title="quaterion.train.encoders.cache_config.CacheConfig" href="encoders/cache_config.html#quaterion.train.encoders.cache_config.CacheConfig">CacheConfig</a>]</span>
</code></dt>
<dd>
<div class="desc"><p>Method to provide cache configuration</p>
<pre><code>    Use this method to define which encoders should cache calculated embeddings and
    what kind of cache they should use.

    Returns:
        Optional[CacheConfig]: cache configuration to be applied if provided, None
            otherwise
    Examples:

    &gt;&gt;&gt; CacheConfig(CacheType.AUTO)
    CacheConfig(
        cache_type=&lt;CacheType.AUTO: 'auto'&gt;,
        mapping={},
        key_extractors={}
    )

    &gt;&gt;&gt; cache_config = CacheConfig(
</code></pre>
<p>&hellip;
mapping={"text_encoder": CacheType.GPU, "image_encoder": CacheType.CPU}
&hellip; )
CacheConfig(
cache_type=None,
mapping={
'text_encoder': <CacheType.GPU: 'gpu'>,
'image_encoder': <CacheType.CPU: 'cpu'>
},
key_extractors={}
)
&gt;&gt;&gt; CacheConfig(
&hellip;
cache_type=CacheType.AUTO,
&hellip;
key_extractors={"default": lambda obj: hash(obj)}
&hellip; )
CacheConfig(
cache_type=<CacheType.AUTO: 'auto'>,
mapping={},
key_extractors={'default': <function \<lambda> at 0x106bc90e0>}
)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def configure_caches(self) -&gt; Optional[CacheConfig]:
    &#34;&#34;&#34;Method to provide cache configuration

            Use this method to define which encoders should cache calculated embeddings and
            what kind of cache they should use.

            Returns:
                Optional[CacheConfig]: cache configuration to be applied if provided, None
                    otherwise
            Examples:

            &gt;&gt;&gt; CacheConfig(CacheType.AUTO)
            CacheConfig(
                cache_type=&lt;CacheType.AUTO: &#39;auto&#39;&gt;,
                mapping={},
                key_extractors={}
            )

            &gt;&gt;&gt; cache_config = CacheConfig(
    ...     mapping={&#34;text_encoder&#34;: CacheType.GPU, &#34;image_encoder&#34;: CacheType.CPU}
    ... )
            CacheConfig(
                cache_type=None,
                mapping={
                    &#39;text_encoder&#39;: &lt;CacheType.GPU: &#39;gpu&#39;&gt;,
                    &#39;image_encoder&#39;: &lt;CacheType.CPU: &#39;cpu&#39;&gt;
                },
                key_extractors={}
            )
            &gt;&gt;&gt; CacheConfig(
    ...     cache_type=CacheType.AUTO,
    ...     key_extractors={&#34;default&#34;: lambda obj: hash(obj)}
    ... )
            CacheConfig(
                cache_type=&lt;CacheType.AUTO: &#39;auto&#39;&gt;,
                mapping={},
                key_extractors={&#39;default&#39;: &lt;function &lt;lambda&gt; at 0x106bc90e0&gt;}
            )

    &#34;&#34;&#34;
    pass</code></pre>
</details>
</dd>
<dt id="quaterion.train.trainable_model.TrainableModel.configure_encoders"><code class="name flex">
<span>def <span class="ident">configure_encoders</span></span>(<span>self) ‑> Union[quaterion_models.encoders.encoder.Encoder, Dict[str, quaterion_models.encoders.encoder.Encoder]]</span>
</code></dt>
<dd>
<div class="desc"><p>Method to provide encoders configuration</p>
<p>Use this function to define an initial state of encoders.
This function should be used to assign initial values for encoders
before training as well as during the checkpoint loading.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Union[Encoder, Dict[str, Encoder]]</code></dt>
<dd>one instance of encoder which will
have quaterion_models.model.DEFAULT_ENCODER_KEY, or dict of encoder
names and corresponding encoder instances.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def configure_encoders(self) -&gt; Union[Encoder, Dict[str, Encoder]]:
    &#34;&#34;&#34;Method to provide encoders configuration

    Use this function to define an initial state of encoders.
    This function should be used to assign initial values for encoders
    before training as well as during the checkpoint loading.

    Returns:
         Union[Encoder, Dict[str, Encoder]]: one instance of encoder which will
            have quaterion_models.model.DEFAULT_ENCODER_KEY, or dict of encoder
            names and corresponding encoder instances.
    &#34;&#34;&#34;
    raise NotImplementedError()</code></pre>
</details>
</dd>
<dt id="quaterion.train.trainable_model.TrainableModel.configure_head"><code class="name flex">
<span>def <span class="ident">configure_head</span></span>(<span>self, input_embedding_size: int) ‑> quaterion_models.heads.encoder_head.EncoderHead</span>
</code></dt>
<dd>
<div class="desc"><p>Use this function to define an initial state for head layer of the model.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>input_embedding_size</code></strong></dt>
<dd>size of embeddings produced by encoders</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>EncoderHead</code></dt>
<dd>instance of EncoderHead to be added to the model</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def configure_head(self, input_embedding_size: int) -&gt; EncoderHead:
    &#34;&#34;&#34;Use this function to define an initial state for head layer of the model.

    Args:
        input_embedding_size: size of embeddings produced by encoders
    Returns:
        EncoderHead: instance of EncoderHead to be added to the model
    &#34;&#34;&#34;
    raise NotImplementedError()</code></pre>
</details>
</dd>
<dt id="quaterion.train.trainable_model.TrainableModel.configure_loss"><code class="name flex">
<span>def <span class="ident">configure_loss</span></span>(<span>self) ‑> <a title="quaterion.loss.similarity_loss.SimilarityLoss" href="../loss/similarity_loss.html#quaterion.loss.similarity_loss.SimilarityLoss">SimilarityLoss</a></span>
</code></dt>
<dd>
<div class="desc"><p>Method to configure loss function to use.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def configure_loss(self) -&gt; SimilarityLoss:
    &#34;&#34;&#34;Method to configure loss function to use.&#34;&#34;&#34;
    raise NotImplementedError()</code></pre>
</details>
</dd>
<dt id="quaterion.train.trainable_model.TrainableModel.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, *args, **kwargs) ‑> Any</span>
</code></dt>
<dd>
<div class="desc"><p>Same as :meth:<code>torch.nn.Module.forward()</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>*args</code></strong></dt>
<dd>Whatever you decide to pass into the forward method.</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Keyword arguments are also possible.</dd>
</dl>
<h2 id="return">Return</h2>
<p>Your model's output</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, *args, **kwargs) -&gt; Any:
    r&#34;&#34;&#34;
    Same as :meth:`torch.nn.Module.forward()`.

    Args:
        *args: Whatever you decide to pass into the forward method.
        **kwargs: Keyword arguments are also possible.

    Return:
        Your model&#39;s output
    &#34;&#34;&#34;
    return super().forward(*args, **kwargs)</code></pre>
</details>
</dd>
<dt id="quaterion.train.trainable_model.TrainableModel.predict_dataloader"><code class="name flex">
<span>def <span class="ident">predict_dataloader</span></span>(<span>self) ‑> Union[torch.utils.data.dataloader.DataLoader, Sequence[torch.utils.data.dataloader.DataLoader]]</span>
</code></dt>
<dd>
<div class="desc"><p>Implement one or multiple PyTorch DataLoaders for prediction.</p>
<p>It's recommended that all data downloads and preparation happen in :meth:<code>prepare_data</code>.</p>
<ul>
<li>:meth:<code>~pytorch_lightning.trainer.Trainer.fit</code></li>
<li>&hellip;</li>
<li>:meth:<code>prepare_data</code></li>
<li>:meth:<code>train_dataloader</code></li>
<li>:meth:<code>val_dataloader</code></li>
<li>:meth:<code>test_dataloader</code></li>
</ul>
<h2 id="note">Note</h2>
<p>Lightning adds the correct sampler for distributed and arbitrary hardware
There is no need to set it yourself.</p>
<h2 id="return">Return</h2>
<p>A :class:<code>torch.utils.data.DataLoader</code> or a sequence of them specifying prediction samples.</p>
<h2 id="note_1">Note</h2>
<p>In the case where you return multiple prediction dataloaders, the :meth:<code>predict</code>
will have an argument <code>dataloader_idx</code> which matches the order here.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict_dataloader(self) -&gt; EVAL_DATALOADERS:
    pass</code></pre>
</details>
</dd>
<dt id="quaterion.train.trainable_model.TrainableModel.process_results"><code class="name flex">
<span>def <span class="ident">process_results</span></span>(<span>self, embeddings: torch.Tensor, targets: Dict[str, Any], batch_idx: int, stage: <a title="quaterion.utils.enums.TrainStage" href="../utils/enums.html#quaterion.utils.enums.TrainStage">TrainStage</a>, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Method to provide any additional evaluations of embeddings.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>embeddings</code></strong></dt>
<dd>shape: (batch_size, embedding_size) - model's output.</dd>
<dt><strong><code>targets</code></strong></dt>
<dd>output of batch target collate.</dd>
<dt><strong><code>batch_idx</code></strong></dt>
<dd>ID of the processing batch.</dd>
<dt><strong><code>stage</code></strong></dt>
<dd>train, validation or test stage.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def process_results(
    self,
    embeddings: Tensor,
    targets: Dict[str, Any],
    batch_idx: int,
    stage: TrainStage,
    **kwargs,
):
    &#34;&#34;&#34;Method to provide any additional evaluations of embeddings.

    Args:
        embeddings: shape: (batch_size, embedding_size) - model&#39;s output.
        targets: output of batch target collate.
        batch_idx: ID of the processing batch.
        stage: train, validation or test stage.
    &#34;&#34;&#34;
    pass</code></pre>
</details>
</dd>
<dt id="quaterion.train.trainable_model.TrainableModel.save_servable"><code class="name flex">
<span>def <span class="ident">save_servable</span></span>(<span>self, path: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Save model for serving, independent of Pytorch Lightning</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>path</code></strong></dt>
<dd>path to save to</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save_servable(self, path: str):
    &#34;&#34;&#34;Save model for serving, independent of Pytorch Lightning

    Args:
        path: path to save to
    &#34;&#34;&#34;
    self.model.save(path)</code></pre>
</details>
</dd>
<dt id="quaterion.train.trainable_model.TrainableModel.test_dataloader"><code class="name flex">
<span>def <span class="ident">test_dataloader</span></span>(<span>self) ‑> Union[torch.utils.data.dataloader.DataLoader, Sequence[torch.utils.data.dataloader.DataLoader]]</span>
</code></dt>
<dd>
<div class="desc"><p>Implement one or multiple PyTorch DataLoaders for testing.</p>
<p>The dataloader you return will not be reloaded unless you set
:paramref:<code>~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_n_epochs</code> to
a postive integer.</p>
<p>For data processing use the following pattern:</p>
<pre><code>- download in :meth:&lt;code&gt;prepare\_data&lt;/code&gt;
- process and split in :meth:&lt;code&gt;setup&lt;/code&gt;
</code></pre>
<p>However, the above are only necessary for distributed processing.</p>
<div class="admonition warning">
<p class="admonition-title">Warning:&ensp;do not assign state in prepare_data</p>
</div>
<ul>
<li>:meth:<code>~pytorch_lightning.trainer.Trainer.fit</code></li>
<li>&hellip;</li>
<li>:meth:<code>prepare_data</code></li>
<li>:meth:<code>setup</code></li>
<li>:meth:<code>train_dataloader</code></li>
<li>:meth:<code>val_dataloader</code></li>
<li>:meth:<code>test_dataloader</code></li>
</ul>
<h2 id="note">Note</h2>
<p>Lightning adds the correct sampler for distributed and arbitrary hardware.
There is no need to set it yourself.</p>
<h2 id="return">Return</h2>
<p>A :class:<code>torch.utils.data.DataLoader</code> or a sequence of them specifying testing samples.</p>
<p>Example::</p>
<pre><code>def test_dataloader(self):
    transform = transforms.Compose([transforms.ToTensor(),
                                    transforms.Normalize((0.5,), (1.0,))])
    dataset = MNIST(root='/path/to/mnist/', train=False, transform=transform,
                    download=True)
    loader = torch.utils.data.DataLoader(
        dataset=dataset,
        batch_size=self.batch_size,
        shuffle=False
    )

    return loader

# can also return multiple dataloaders
def test_dataloader(self):
    return [loader_a, loader_b, ..., loader_n]
</code></pre>
<h2 id="note_1">Note</h2>
<p>If you don't need a test dataset and a :meth:<code>test_step</code>, you don't need to implement
this method.</p>
<h2 id="note_2">Note</h2>
<p>In the case where you return multiple test dataloaders, the :meth:<code>test_step</code>
will have an argument <code>dataloader_idx</code> which matches the order here.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_dataloader(self) -&gt; EVAL_DATALOADERS:
    pass</code></pre>
</details>
</dd>
<dt id="quaterion.train.trainable_model.TrainableModel.test_step"><code class="name flex">
<span>def <span class="ident">test_step</span></span>(<span>self, batch, batch_idx, **kwargs) ‑> Optional[torch.Tensor]</span>
</code></dt>
<dd>
<div class="desc"><p>Compute test loss and some additional metrics for e.g. the progress
bar or logger.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>batch</code></strong></dt>
<dd>Output of DataLoader.</dd>
<dt><strong><code>batch_idx</code></strong></dt>
<dd>Integer displaying index of this batch.</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>keyword arguments to be passed into <code>process_results</code></dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_step(self, batch, batch_idx, **kwargs) -&gt; Optional[Tensor]:
    &#34;&#34;&#34;Compute test loss and some additional metrics for e.g. the progress
    bar or logger.

    Args:
        batch: Output of DataLoader.
        batch_idx: Integer displaying index of this batch.
        **kwargs: keyword arguments to be passed into `process_results`
    &#34;&#34;&#34;
    stage = TrainStage.TEST
    self._common_step(batch=batch, batch_idx=batch_idx, stage=stage, **kwargs)
    return None</code></pre>
</details>
</dd>
<dt id="quaterion.train.trainable_model.TrainableModel.train_dataloader"><code class="name flex">
<span>def <span class="ident">train_dataloader</span></span>(<span>self) ‑> Union[torch.utils.data.dataloader.DataLoader, Sequence[torch.utils.data.dataloader.DataLoader], Sequence[Sequence[torch.utils.data.dataloader.DataLoader]], Sequence[Dict[str, torch.utils.data.dataloader.DataLoader]], Dict[str, torch.utils.data.dataloader.DataLoader], Dict[str, Dict[str, torch.utils.data.dataloader.DataLoader]], Dict[str, Sequence[torch.utils.data.dataloader.DataLoader]]]</span>
</code></dt>
<dd>
<div class="desc"><p>Implement one or more PyTorch DataLoaders for training.</p>
<h2 id="return">Return</h2>
<p>A collection of :class:<code>torch.utils.data.DataLoader</code> specifying training samples.
In the case of multiple dataloaders, please see this :ref:<code>page &lt;multiple-training-dataloaders&gt;</code>.</p>
<p>The dataloader you return will not be reloaded unless you set
:paramref:<code>~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_n_epochs</code> to
a positive integer.</p>
<p>For data processing use the following pattern:</p>
<pre><code>- download in :meth:&lt;code&gt;prepare\_data&lt;/code&gt;
- process and split in :meth:&lt;code&gt;setup&lt;/code&gt;
</code></pre>
<p>However, the above are only necessary for distributed processing.</p>
<div class="admonition warning">
<p class="admonition-title">Warning:&ensp;do not assign state in prepare_data</p>
</div>
<ul>
<li>:meth:<code>~pytorch_lightning.trainer.Trainer.fit</code></li>
<li>&hellip;</li>
<li>:meth:<code>prepare_data</code></li>
<li>:meth:<code>setup</code></li>
<li>:meth:<code>train_dataloader</code></li>
</ul>
<h2 id="note">Note</h2>
<p>Lightning adds the correct sampler for distributed and arbitrary hardware.
There is no need to set it yourself.</p>
<p>Example::</p>
<pre><code># single dataloader
def train_dataloader(self):
    transform = transforms.Compose([transforms.ToTensor(),
                                    transforms.Normalize((0.5,), (1.0,))])
    dataset = MNIST(root='/path/to/mnist/', train=True, transform=transform,
                    download=True)
    loader = torch.utils.data.DataLoader(
        dataset=dataset,
        batch_size=self.batch_size,
        shuffle=True
    )
    return loader

# multiple dataloaders, return as list
def train_dataloader(self):
    mnist = MNIST(...)
    cifar = CIFAR(...)
    mnist_loader = torch.utils.data.DataLoader(
        dataset=mnist, batch_size=self.batch_size, shuffle=True
    )
    cifar_loader = torch.utils.data.DataLoader(
        dataset=cifar, batch_size=self.batch_size, shuffle=True
    )
    # each batch will be a list of tensors: [batch_mnist, batch_cifar]
    return [mnist_loader, cifar_loader]

# multiple dataloader, return as dict
def train_dataloader(self):
    mnist = MNIST(...)
    cifar = CIFAR(...)
    mnist_loader = torch.utils.data.DataLoader(
        dataset=mnist, batch_size=self.batch_size, shuffle=True
    )
    cifar_loader = torch.utils.data.DataLoader(
        dataset=cifar, batch_size=self.batch_size, shuffle=True
    )
    # each batch will be a dict of tensors: {'mnist': batch_mnist, 'cifar': batch_cifar}
    return {'mnist': mnist_loader, 'cifar': cifar_loader}
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train_dataloader(self) -&gt; TRAIN_DATALOADERS:
    pass</code></pre>
</details>
</dd>
<dt id="quaterion.train.trainable_model.TrainableModel.training_step"><code class="name flex">
<span>def <span class="ident">training_step</span></span>(<span>self, batch: Union[torch.Tensor, Tuple[torch.Tensor], List[torch.Tensor], Dict[str, torch.Tensor], Dict[str, dict], Any], batch_idx: int, **kwargs) ‑> torch.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Compute and return the training loss and some additional metrics for e.g.
the progress bar or logger.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>batch</code></strong></dt>
<dd>Output of DataLoader.</dd>
<dt><strong><code>batch_idx</code></strong></dt>
<dd>Integer displaying index of this batch.</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>keyword arguments to be passed into <code>process_results</code></dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Tensor</code></dt>
<dd>computed loss value</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def training_step(
    self, batch: TensorInterchange, batch_idx: int, **kwargs
) -&gt; Tensor:
    &#34;&#34;&#34;Compute and return the training loss and some additional metrics for e.g.
    the progress bar or logger.

    Args:
        batch: Output of DataLoader.
        batch_idx: Integer displaying index of this batch.
        **kwargs: keyword arguments to be passed into `process_results`

    Returns:
        Tensor: computed loss value
    &#34;&#34;&#34;
    stage = TrainStage.TRAIN
    loss = self._common_step(
        batch=batch, batch_idx=batch_idx, stage=stage, **kwargs
    )
    return loss</code></pre>
</details>
</dd>
<dt id="quaterion.train.trainable_model.TrainableModel.val_dataloader"><code class="name flex">
<span>def <span class="ident">val_dataloader</span></span>(<span>self) ‑> Union[torch.utils.data.dataloader.DataLoader, Sequence[torch.utils.data.dataloader.DataLoader]]</span>
</code></dt>
<dd>
<div class="desc"><p>Implement one or multiple PyTorch DataLoaders for validation.</p>
<p>The dataloader you return will not be reloaded unless you set
:paramref:<code>~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_n_epochs</code> to
a positive integer.</p>
<p>It's recommended that all data downloads and preparation happen in :meth:<code>prepare_data</code>.</p>
<ul>
<li>:meth:<code>~pytorch_lightning.trainer.Trainer.fit</code></li>
<li>&hellip;</li>
<li>:meth:<code>prepare_data</code></li>
<li>:meth:<code>train_dataloader</code></li>
<li>:meth:<code>val_dataloader</code></li>
<li>:meth:<code>test_dataloader</code></li>
</ul>
<h2 id="note">Note</h2>
<p>Lightning adds the correct sampler for distributed and arbitrary hardware
There is no need to set it yourself.</p>
<h2 id="return">Return</h2>
<p>A :class:<code>torch.utils.data.DataLoader</code> or a sequence of them specifying validation samples.</p>
<p>Examples::</p>
<pre><code>def val_dataloader(self):
    transform = transforms.Compose([transforms.ToTensor(),
                                    transforms.Normalize((0.5,), (1.0,))])
    dataset = MNIST(root='/path/to/mnist/', train=False,
                    transform=transform, download=True)
    loader = torch.utils.data.DataLoader(
        dataset=dataset,
        batch_size=self.batch_size,
        shuffle=False
    )

    return loader

# can also return multiple dataloaders
def val_dataloader(self):
    return [loader_a, loader_b, ..., loader_n]
</code></pre>
<h2 id="note_1">Note</h2>
<p>If you don't need a validation dataset and a :meth:<code>validation_step</code>, you don't need to
implement this method.</p>
<h2 id="note_2">Note</h2>
<p>In the case where you return multiple validation dataloaders, the :meth:<code>validation_step</code>
will have an argument <code>dataloader_idx</code> which matches the order here.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def val_dataloader(self) -&gt; EVAL_DATALOADERS:
    pass</code></pre>
</details>
</dd>
<dt id="quaterion.train.trainable_model.TrainableModel.validation_step"><code class="name flex">
<span>def <span class="ident">validation_step</span></span>(<span>self, batch, batch_idx, **kwargs) ‑> Optional[torch.Tensor]</span>
</code></dt>
<dd>
<div class="desc"><p>Compute validation loss and some additional metrics for e.g. the progress
bar or logger.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>batch</code></strong></dt>
<dd>Output of DataLoader.</dd>
<dt><strong><code>batch_idx</code></strong></dt>
<dd>Integer displaying index of this batch.</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>keyword arguments to be passed into <code>process_results</code></dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def validation_step(self, batch, batch_idx, **kwargs) -&gt; Optional[Tensor]:
    &#34;&#34;&#34;Compute validation loss and some additional metrics for e.g. the progress
    bar or logger.

    Args:
        batch: Output of DataLoader.
        batch_idx: Integer displaying index of this batch.
        **kwargs: keyword arguments to be passed into `process_results`
    &#34;&#34;&#34;
    stage = TrainStage.VALIDATION
    self._common_step(batch=batch, batch_idx=batch_idx, stage=stage, **kwargs)
    return None</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="quaterion.train.cache_mixin.CacheMixin" href="cache_mixin.html#quaterion.train.cache_mixin.CacheMixin">CacheMixin</a></b></code>:
<ul class="hlist">
<li><code><a title="quaterion.train.cache_mixin.CacheMixin.cache" href="cache_mixin.html#quaterion.train.cache_mixin.CacheMixin.cache">cache</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="quaterion.train" href="index.html">quaterion.train</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="quaterion.train.trainable_model.TrainableModel" href="#quaterion.train.trainable_model.TrainableModel">TrainableModel</a></code></h4>
<ul class="two-column">
<li><code><a title="quaterion.train.trainable_model.TrainableModel.configure_caches" href="#quaterion.train.trainable_model.TrainableModel.configure_caches">configure_caches</a></code></li>
<li><code><a title="quaterion.train.trainable_model.TrainableModel.configure_encoders" href="#quaterion.train.trainable_model.TrainableModel.configure_encoders">configure_encoders</a></code></li>
<li><code><a title="quaterion.train.trainable_model.TrainableModel.configure_head" href="#quaterion.train.trainable_model.TrainableModel.configure_head">configure_head</a></code></li>
<li><code><a title="quaterion.train.trainable_model.TrainableModel.configure_loss" href="#quaterion.train.trainable_model.TrainableModel.configure_loss">configure_loss</a></code></li>
<li><code><a title="quaterion.train.trainable_model.TrainableModel.dump_patches" href="#quaterion.train.trainable_model.TrainableModel.dump_patches">dump_patches</a></code></li>
<li><code><a title="quaterion.train.trainable_model.TrainableModel.forward" href="#quaterion.train.trainable_model.TrainableModel.forward">forward</a></code></li>
<li><code><a title="quaterion.train.trainable_model.TrainableModel.loss" href="#quaterion.train.trainable_model.TrainableModel.loss">loss</a></code></li>
<li><code><a title="quaterion.train.trainable_model.TrainableModel.model" href="#quaterion.train.trainable_model.TrainableModel.model">model</a></code></li>
<li><code><a title="quaterion.train.trainable_model.TrainableModel.predict_dataloader" href="#quaterion.train.trainable_model.TrainableModel.predict_dataloader">predict_dataloader</a></code></li>
<li><code><a title="quaterion.train.trainable_model.TrainableModel.process_results" href="#quaterion.train.trainable_model.TrainableModel.process_results">process_results</a></code></li>
<li><code><a title="quaterion.train.trainable_model.TrainableModel.save_servable" href="#quaterion.train.trainable_model.TrainableModel.save_servable">save_servable</a></code></li>
<li><code><a title="quaterion.train.trainable_model.TrainableModel.test_dataloader" href="#quaterion.train.trainable_model.TrainableModel.test_dataloader">test_dataloader</a></code></li>
<li><code><a title="quaterion.train.trainable_model.TrainableModel.test_step" href="#quaterion.train.trainable_model.TrainableModel.test_step">test_step</a></code></li>
<li><code><a title="quaterion.train.trainable_model.TrainableModel.train_dataloader" href="#quaterion.train.trainable_model.TrainableModel.train_dataloader">train_dataloader</a></code></li>
<li><code><a title="quaterion.train.trainable_model.TrainableModel.training" href="#quaterion.train.trainable_model.TrainableModel.training">training</a></code></li>
<li><code><a title="quaterion.train.trainable_model.TrainableModel.training_step" href="#quaterion.train.trainable_model.TrainableModel.training_step">training_step</a></code></li>
<li><code><a title="quaterion.train.trainable_model.TrainableModel.val_dataloader" href="#quaterion.train.trainable_model.TrainableModel.val_dataloader">val_dataloader</a></code></li>
<li><code><a title="quaterion.train.trainable_model.TrainableModel.validation_step" href="#quaterion.train.trainable_model.TrainableModel.validation_step">validation_step</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>